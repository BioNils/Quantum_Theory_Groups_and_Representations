%!TEX TS-program = lualatex
\documentclass[11pt,a4paper]{article}
\usepackage[english]{babel} 
%LTeX: language=en-US
\usepackage[no-math]{fontspec}
%\usepackage{stmaryrd}
\usepackage{cite}
%\SetSymbolFont{stmry}{bold}{U}{stmry}{m}{n}
% margin
\setlength{\topmargin}{0.4cm}
\setlength{\headheight}{0.4cm}
\setlength{\headsep}{0.8cm}
\setlength{\footskip}{1cm}
\setlength{\textwidth}{17cm}
\setlength{\textheight}{24cm}
\setlength{\voffset}{-1.5cm}
\setlength{\hoffset}{-0.5cm}
\setlength{\oddsidemargin}{0cm}
\setlength{\evensidemargin}{0cm}
\usepackage{lipsum} 
\usepackage[]{mdframed}
\usepackage{physics} 
\usepackage{siunitx} % units
\DeclareSIUnit{\octet}{o}
\usepackage{svg}
\svgsetup{
    inkscape=pdf
}
\RequirePackage{lastpage}
\usepackage{appendix}
\usepackage{cite}
\usepackage{float}
\usepackage{multicol}
\usepackage{graphicx}			
\usepackage{amsmath}				
\usepackage{amssymb}			
\usepackage{amsfonts}
\usepackage{slashed}
\usepackage{subcaption}
\usepackage{fourier-orns}          		
\usepackage[T1]{fontenc}
\usepackage{tabularx}				
\usepackage{stmaryrd} 
\usepackage{psfrag}	
\usepackage[strict]{changepage}
\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[section]
\newtheorem*{remark}{Remark}
\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\makeatletter
\DeclareSizeFunction{sub}{\sub@sfcnt\@font@info}
\makeatother
\usepackage{color}
\usepackage{cancel}			
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage[labelfont=bf,format=plain,justification=raggedright]{caption}
\definecolor{linkcolor}{rgb}{0,0,0.6}		
\usepackage[colorlinks=true,	
			pdfstartview=FitV,
			linkcolor= linkcolor,
			citecolor= linkcolor,
			urlcolor= linkcolor,
			hyperindex=true,
			hyperfigures=false,
			linktocpage=true,
			pdfproducer=medialab,
			pdfa=true
			]
			{hyperref}				
\usepackage{fancyhdr}
\usepackage{tikz}
\usetikzlibrary{arrows.meta,bending,positioning}
\usepackage[compat=1.1.0]{tikz-feynman}	
\usepackage[most]{tcolorbox}
\definecolor{ShadowColor}{RGB}{30,150,190}
\makeatletter
\newcommand\Cshadowbox{\VerbBox\@Cshadowbox}
\def\@Cshadowbox#1{%
  \setbox\@fancybox\hbox{\fbox{#1}}%
  \leavevmode\vbox{%
    \offinterlineskip
    \dimen@=\shadowsize
    \advance\dimen@ .5\fboxrule
    \hbox{\copy\@fancybox\kern.5\fboxrule\lower\shadowsize\hbox{%
      \color{ShadowColor}\vrule \@height\ht\@fancybox \@depth\dp\@fancybox \@width\dimen@}}%
    \vskip\dimexpr-\dimen@+0.5\fboxrule\relax
    \moveright\shadowsize\vbox{%
      \color{ShadowColor}\hrule \@width\wd\@fancybox \@height\dimen@}}}
\makeatother
\makeatletter
\@ifpackageloaded{babel}%
        {\newcommand{\nospace}[1]{{\NoAutoSpaceBeforeFDP{}#1}}}
        {\newcommand{\nospace}[1]{#1}}
\makeatother
\newcommand{\drawat}[3]{\makebox[0pt][l]{\raisebox{#2}{\hspace*{#1}#3}}}

\AtBeginDocument{\RenewCommandCopy{\qty}{\SI}}
\setlength{\headheight}{28.3 pt}
\numberwithin{equation}{section}
\newcommand{\pd}[1]{\partial_{#1}}
\newcommand{\pu}[1]{\partial^{#1}}
%%%%%%%%%%%%%% Color Macro
\usepackage{xcolor}
\makeatletter
\def\mathcolor#1#{\@mathcolor{#1}}
\def\@mathcolor#1#2#3{%
  \protect\leavevmode
  \begingroup
    \color#1{#2}#3%
  \endgroup
}
\makeatother
\title{Quantum Theory, Groups and Representations}
\begin{document}
\maketitle
\section{Representation theory}
\begin{definition}
    A representation $\left(\pi,V\right)$ of a group $G$ over a vector space $V$ is an application $\pi:G\rightarrow GL(V)$ such that $\forall g_1,g_2 \in G, \ \pi\left(g_1\cdot g_2\right) = \pi\left(g_1\right) \pi(g_2)$.
    \begin{itemize}
        \item A representation $(\pi,V)$ is said \textit{irreducible} if there is no proper subset $W \subset  V$ such that $\pi_W:G\rightarrow GL(W)$ is a representation.\\
         Another way to express that is that the only invariant\footnote{A subspace $V'\subseteq V $ is called invariant if $\pi(g)V' \subseteq V'$ for all $g \in G$.} subvector spaces of the representation are $\{0\}$ and $V$.
        \item Otherwise, $(\pi,V)$ is said \textit{reducible}.
    \end{itemize}
\end{definition}
\begin{definition}
    A unitary representation $(\pi,V)$, where $V$ is a $\mathbb{K}$-EV equiped with an inner product $\expval{\ ,\ }:V\cross V \rightarrow \mathbb{K}$, is a representation which preserves the inner product such that:
    \begin{equation*}
        \forall g \in G,\ \forall v,w \in V,\ \expval{v,w} = \expval{\pi(g)v, \pi(g)w}. 
    \end{equation*}
    If $V$ is finite dimensionnal (of dimension $n$), we can treat the representation as an element of $\mathcal{M}_{n \cross n}\left(\mathbb{K}\right)$, and for a unitary transformation, $\pi : G \rightarrow U(n)$.
\end{definition}
\begin{definition}
    (Direct sum representation). Given representations $\pi_1$ and $\pi_2$ of dimensions $n_1$ and $n_2$, there is a representation of dimension $n_1+n_2$ called the direct sum of the two representations, denoted $\pi_1 \oplus \pi_2$. This representation is given by the homomorphism:
    \begin{equation*}
        \left(\pi_1 \oplus \pi_2\right):g \in G \mapsto \begin{pmatrix}
            \pi_1(g)& \vb{0}\\
            \vb{0}& \pi_2(g)
        \end{pmatrix}.
    \end{equation*}
\end{definition}
\begin{theorem}
    A unitary transformation $\pi$ over a finite dimensionnal vector space $V$ can be written as $\oplus \pi_i $, where $\left\{\pi_i\right\}$ is a set of irreducible representations.
\end{theorem}
\begin{proof}
    If $(\pi,V)$ is irreducible, then it's obvious.\\
    Otherwise, there exists $W \subset V$ such that $(\pi_W,W)$ is a representation.  \\
    Since $V = W  \oplus W^\perp$, we want to check if $(\pi_{W^\perp},W^\perp)$ is a representation.
    \begin{itemize}
        \item Let $w \in W$ and $v \in W^\perp$, then obviously $v \perp w \Leftrightarrow \expval{v,w} = \expval{\pi(g)v,\pi(g)w}= 0$. Since $(\pi_W,W)$ is a representation, $\pi(g)w \in W$ and thus $\forall g \in G,\ \pi(g)v \perp W $. \\
        This means that $\forall g \in G,\ \forall v \in W^\perp,\ \pi(g)v \in W^\perp$. Thus, $\pi_{W^\perp},W^\perp$ is a representation.(It respects the group product since $(\pi,V)$ is a representation.)
    \end{itemize}
    Thus:\begin{equation*}
        (\pi,V) = (\pi_W,W)\oplus(\pi_{W^\perp},W^\perp).
    \end{equation*}
    We can then repeat this process until each subrepresentation is irreducible.
\end{proof}
\begin{theorem}{Schur's Lemna:}
    If a \textbf{complex} representation $(\pi,V)$ is irreducible, then the only application $M:V\rightarrow V$ commuting with all the $\pi(g)$ is $M = \lambda \mathbb{I},\ \lambda \in \mathbb{C}$. 
\end{theorem}
\begin{proof}
    Let's assume that $(\pi,V)$ is an irreducible representation and that $\forall g \in G, \left[M,\pi(g)\right] = 0$. Since $V$ is a $\mathbb{C}$-vector space, the equation:
    \begin{equation*}
        \det(M-\lambda \mathbb{I}) = 0
    \end{equation*}
    has $n$ roots (counting the multiplicity) and thus we can find all the eigenspaces:
    \begin{equation*}
        U_\lambda = \left\{v\in V | Mv = \lambda v\right\} = \ker\left(M-\lambda \mathbb{I}\right) \neq  \emptyset .
    \end{equation*}
    Since $\pi$ and $M$ commute, $v\in\ker\left(M-\lambda \mathbb{I}\right) \implies  \forall g \in G\ \pi(g)v \in \ker\left(M-\lambda \mathbb{I}\right) $.
    As a consequence, $(\pi,\ker\left(M-\lambda \mathbb{I}\right))$ is a representation of $G$ since $\ker\left(M-\lambda \mathbb{I}\right)$ is stable under the action of $\pi$.
    Since we supposed that $(\pi,V)$ is irreducible then either:
    \begin{itemize}
        \item $U_\lambda = \ker\left(M-\lambda \mathbb{I}\right) = \emptyset$, but it's not possible since $U_\lambda$ is an eignespace.
        \item $U_\lambda = \ker\left(M-\lambda \mathbb{I}\right) = V \Leftrightarrow \forall v \in V, (M-\lambda \mathbb{I})v=0$, which implies that $M = \lambda \mathbb{I}$.
    \end{itemize}
\end{proof}
\begin{theorem}
    If $G$ is an abelian group, then all of its irreducible representations are one dimensionnal.
\end{theorem}
\begin{proof}
    Consider an abelian (=commutative) group $G$ and $g \in G$.
    Any representation $(\pi,V)$ of $G$ will satisfy:
    \begin{equation*}
       \forall h \in G,\  \pi(g) \pi(h) = \pi(h) \pi(g).
    \end{equation*}
and if $\pi$ is irreducible, by using Schur's Lemna, $\pi(h) = \lambda \mathbb{I} \simeq \lambda$ for $\lambda \in \mathbb{C}$.
\end{proof}
\section{Lie Algebra}
The main idea is that a representation $\pi$ is homomorphic and this proporty allows to charachterize $\pi$ only in term of its derivative $\pi'$. $\pi'$ is a linear map from the tangent space to $G$ at the identity to the tangent space of $U(n)$ at the identity. The tangent space to $G$ at the identity will carry some extra structure coming from the group law and this \textit{vector space} with this structure will be called the Lie algebra of $G$, while $\pi'$ will be an example of a Lie algebra representation.
\begin{definition}[Lie Group] 
    A Lie group $G$ is a differentiable manifold equiped with a group law. 
\end{definition}
\begin{definition}[Lie Algebra]
    The Lie Algebra of a Lie group $G$ is the tangent space of $G$ at the identity.
\end{definition}
\begin{definition}[Lie Algebra of a matrix group $G$]
    For $G$ a Lie group of $n\times n$ invertible matrices, the Lie algebra of $G$ (written $Lie(G)$ or $\mathfrak{g}$) is the space $X$ of $n\times n$ matrices such that $e^{tX}\in G$ for $t \in \mathbb{R}$.
\end{definition}
We have defined the exponential of the matrices throug its power serie
\begin{equation}
    e^A = \mathbf{1} + A + \frac{1}{2} A^2 + \cdots + \frac{1}{n!}A^n + \cdots
\end{equation}
which can be shown to converge for any matrix $A$.
\begin{proof}
    We define the \textit{matrix absolute value}\footnote{One can show that it is a norm just by identifying the isomorphism between $M_n(\mathbb{K})$ and $\mathbb{K}^{n^2}$ equiped with the canonical norm, and since we are in a finite dimensionnal vector space, all norms are equivalent.} of \( A = (a_{ij}) \) to be

\begin{equation*}
|A| = \sqrt{\sum_{i,j} |a_{ij}|^2}.
\end{equation*}

For an \( n \times n \) real matrix \( A \) the absolute value \( |A| \) is the distance from the origin \( O \) in \( \mathbb{R}^{n^2} \) of the point

\begin{equation*}
(a_{11}, a_{12}, \dots, a_{1n}, a_{21}, a_{22}, \dots, a_{2n}, \dots, a_{n1}, \dots, a_{nn}).
\end{equation*}

If \( A \) has complex entries, and if we interpret each copy of \( \mathbb{C} \) as \( \mathbb{R}^2 \), then \( |A| \) is the distance from \( O \) of the corresponding point in \( \mathbb{R}^{(2n)^2} \). Similarly, if \( A \) has quaternion entries, then \( |A| \) is the distance from \( O \) of the corresponding point in \( \mathbb{R}^{(4n)^2} \).

In all cases, \( |A - B| \) is the distance between the matrices \( A \) and \( B \), and we say that a sequence \( A_1, A_2, A_3, \dots \) of \( n \times n \) matrices has \textit{limit} \( A \) if, for each \( \varepsilon > 0 \), there is an integer \( M \) such that

\begin{equation*}
m > M \implies |A_m - A| < \varepsilon.
\end{equation*}

The key property of the matrix absolute value is the following inequality, a consequence of the triangle inequality (which holds in the plane and hence in any \( \mathbb{R}^k \)) and the  Cauchy-Schwarz inequality (For all $u,v \in E$ a vector space with a scalar product, we have :$\abs{\expval{u,v}}^2 \leq \expval{u,u}\expval{v,v}$).

\textbf{Submultiplicative property.} \textit{For any two real \( n \times n \) matrices \( A \) and \( B \),}
\begin{equation*}
|AB| \leq |A||B|.
\end{equation*}

\textbf{Proof.} If \( A = (a_{ij}) \) and \( B = (b_{ij}) \), then it follows from the definition of matrix product that

\begin{equation*}
|(i,j)\text{-entry of } AB| = |a_{i1} b_{1j} + a_{i2} b_{2j} + \dots + a_{in} b_{nj}|
\end{equation*}
\begin{equation*}
\leq |a_{i1} b_{1j}| + |a_{i2} b_{2j}| + \dots + |a_{in} b_{nj}|
\end{equation*}
by the triangle inequality

\begin{equation*}
= |a_{i1}||b_{1j}| + |a_{i2}||b_{2j}| + \dots + |a_{in}||b_{nj}|
\end{equation*}
by the multiplicative property of absolute value

\begin{equation*}
\leq \sqrt{|a_{i1}|^2 + \dots + |a_{in}|^2} \sqrt{|b_{1j}|^2 + \dots + |b_{nj}|^2}
\end{equation*}
by the Cauchy–Schwarz inequality.

Now, summing the squares of both sides, we get

\begin{equation*}
|AB|^2 = \sum_{i,j} |(i,j)\text{-entry of } AB|^2
\end{equation*}

\begin{equation*}
\leq \sum_{i,j} (|a_{i1}|^2 + \dots + |a_{in}|^2)(|b_{1j}|^2 + \dots + |b_{nj}|^2)
\end{equation*}

\begin{equation*}
= \sum_{i} (|a_{i1}|^2 + \dots + |a_{in}|^2) \sum_{j} (|b_{1j}|^2 + \dots + |b_{nj}|^2)
\end{equation*}

\begin{equation*}
= |A|^2 |B|^2, \quad \text{as required.} \quad \square
\end{equation*}

It follows from the submultiplicative property that \( |A^m| \leq |A|^m \). Along with the \textit{triangle inequality} \( |A + B| \leq |A| + |B| \), the submultiplicative property enables us to test convergence of matrix infinite series by comparing them with series of real numbers. In particular, we have:

\textbf{Convergence of the exponential series.} \textit{If \( A \) is any \( n \times n \) real matrix, then}
\begin{equation*}
1 + \frac{A}{1!} + \frac{A^2}{2!} + \frac{A^3}{3!} + \dots, \quad \text{where } 1 = n \times n \text{ identity matrix,}
\end{equation*}
\textit{is convergent in \( \mathbb{R}^{n^2} \).}

\textbf{Proof.} It suffices to prove that this series is absolutely convergent, that is, to prove the convergence of

\begin{equation*}
|1| + \frac{|A|}{1!} + \frac{|A^2|}{2!} + \frac{|A^3|}{3!} + \dots.
\end{equation*}

This is a series of positive real numbers, whose terms (except for the first) are less than or equal to the corresponding terms of

\begin{equation*}
1 + \frac{|A|}{1!} + \frac{|A|^2}{2!} + \frac{|A|^3}{3!} + \dots
\end{equation*}
by the submultiplicative property. The latter series is the series for the real exponential function \( e^{|A|} \); hence the original series is convergent.
\end{proof}
Notice that while the group $G$ determines the Lie algebra $\mathfrak{g}$, the Lie algebra does not determine the group. For example $O(n)$ and $SO(n)$ admit the same tangent space at identity and thus the same Lie algebra, but elements in $O(n)$ not in the component of the identity (ie with determinant $-1$) cannot be written in the form $e^{tX}$ since then you could make a path of matrixes connecting such element to the identity by shrinking $t$ to zero.
Note also that for a given $X$, different values of $t$ could give the same group element.
\begin{definition}[Adjoint representation]
    The adjoint representation (Ad, $\mathfrak{g}$) is given by the homomorphism
    \begin{equation*}
        Ad:g\in G \rightarrow Ad(g)\in GL(\mathfrak{g})
    \end{equation*}
    where $Ad(g)$ acts on $X\in \mathfrak{g}$ by
    \begin{equation*}
        (Ad(g))(X) = gXg^{-1}
    \end{equation*}
    To show that this is well defined, one needs to check that $gXg^{-1} \in mathfrak{g}$ when $X \in mathfrak{g}$ but it is guarenteed by the identity
    \begin{equation*}
        e^{tgXg^{-1}} = ge^{tX}g^{-1}
    \end{equation*}
    obtained directly from the serie representation of the exponential.
    The homomorphic property of $Ad$ is also easy to show.
\end{definition}
A Lie algebra is not just a \textit{real} vector space, it comes with an additional structure:
\begin{definition}[Lie Bracket]
    The Lie bracket operation on $ \mathfrak{g}$ is the bilinear antisymetric map given by the commutator of matrices
    \begin{equation*}
        \left[\cdot,\cdot\right]:(X,Y)\in \mathfrak{g} \times \mathfrak{g} \rightarrow \left[X,Y\right] = XY -YX \in \mathfrak{g}
    \end{equation*}
\end{definition}
We now have to check that it is well defined, ie that $\left[X,Y\right]\in \mathfrak{g}$.
\begin{theorem}
    If $X,Y \in  \mathfrak{g}, \left[X,Y\right] = XY - YX \in \mathfrak{g}$.
\end{theorem}
\begin{proof}
    Since $X \in \mathfrak{g}$, we have $e^{tX} \in G$ and we can act on $Y \in \mathfrak{g}$ with the adjoint representation
    \begin{equation*}
        Ad(e^{tX})Y = e^{tX}Ye^{-tX} \in \mathfrak{g}
    \end{equation*}
As $t$ varies, this gives us a parametrized curve in $\mathfrak{g}$, whose derivative is also in $\mathfrak{g}$:
\begin{equation*}
    \dv{t} (e^{tX}Ye^{-tX}) \in \mathfrak{g}.
\end{equation*}
Computing the derivative with the product rule gives:
\begin{equation*}
    \dv{t} (e^{tX}Ye^{-tX}) = X e^{tX}Ye^{-tX} - e^{tX}Ye^{-tX}X
\end{equation*}
and evaluating it a $t=0$, we end up with:
\begin{equation*}
    \dv{t} (e^{tX}Ye^{-tX})|_{t=0} = XY - YX \in \mathfrak{g}.
\end{equation*}
\end{proof}
\begin{remark}
    The relation \begin{equation*}
        \dv{t}\eval{(e^{tX}Ye^{-tX})}_{t=0} = \left[X,Y\right]
    \end{equation*}
    used in this proof is continually useful to relate the Lie group and the Lie algebra.
\end{remark}
\begin{definition}[Structure constants]
Since $\mathfrak{g}$ is a vector space of dimension $n$, one can choose a basis $X_1,\cdots,X_n$ and use the fact that the Lie bracket can be written as:
\begin{equation*}
    \left[X_i,X_j\right] = \sum_{k=1}^{n}c_{ijk}X_k.
\end{equation*}
The set of $c_{ijk}$ is the set of structure constants of the algebra, that are antisymetric in $i\leftrightarrows j$ due to the antisymetry of the Lie bracket.   
\end{definition}
\begin{remark}[Structure constants of $\mathfrak{su}(2)$]
    $\mathfrak{su}(2)$ has a basis $X_1,X_2,X_3$ satisfying
    \begin{equation*}
        \left[X_i,X_j\right] = \sum_{k=1}^{3}\epsilon_{ijk}X_k,
    \end{equation*}
    where $\epsilon_{ijk}$ is the Levi-Cevita antisymetric tensor.
\end{remark}
We will now focus ourself on the groups of linear transformations that preserve an inner product: The unitary and orthogonal groups. These groups are subgroups of respectively $GL(n,\mathbb{C})$ and $GL(n,\mathbb{R})$, consisting of elements $\Omega$ satisfying:
\begin{equation}
    \label{eq:unitary}
    \Omega\Omega^{\dagger,T} = 1
\end{equation}
\subsection{Lie Algebra of the Unitary/Orthogonal Group $U(n)$ and $ O(n)$}
Let $X$ be an element of the Lie algebra $\mathfrak{u}(n)$ such that $\Omega = e^{tX}$ for some $t$.
The condition \ref{eq:unitary} then becomes:
\begin{equation*}
    e^{tX}e^{tX^\dagger} = 1.
\end{equation*}
Taking the derivative of the last equation and evaluating it at $t=0$ gives:
\begin{equation*}
    X + X^\dagger = 0 \Leftrightarrow X = -X^\dagger.
\end{equation*}
\begin{definition}[$\mathfrak{u}(n)$]
    The lie Algebra $\mathfrak{u}(n)$ is the set of $n\times n$ skew-adjoint complex matrices.
\end{definition}

\begin{definition}[$\mathfrak{o}(n)$]
    The Lie Algebra $\mathfrak{o}(n)$ is the setis the set of  anti-symmetric $n\times n$ real matrices $X = - X^T \quad \forall X \in \mathfrak{o}(n)$.
\end{definition}
Regarding the special orthogonal/unitary group, we can use the following identity:
\begin{equation*}
    \det e^{X} = e^{\text{Tr}(X)}.
\end{equation*}
Since the determinant of the matrices of $SU(n)$ is $1$, it add the conditions of tracelessness for the lie Algebra  $\mathfrak{su}(n)$. It is slightly different for $\mathfrak{so}(n)$ since matrices $X$ of $\mathfrak{o}(n)$ are already traceless because of $X^T = -X$ implying that the diagonal coefficients are zero.
\subsection{To remember}
\begin{itemize}
    \item The group of linear application $GL(n,\mathbb{K})$ admit $\mathfrak{gl}(n, \mathbb{K}) = M_{n\times n}(\mathbb{K})$ the space of n by n $\mathbb{K}$ matrices as its Lie Algebra.
    \item The Lie algebra $\mathfrak{sl}(n,\mathbb{K})$ of $SL(n,\mathbb{K})$ the group of inversibe  $\mathbb{K}$-matrices of determinant $1$ is the set of traceless $\mathbb{K}$-matrices.
    \item The orthogonal group $O(n) \subset  GL(n, \mathbb{R})$ is the group of $n \times n$ real matrices $\Omega$ satisfying $\Omega^T = \Omega^{-1}$. Its Lie algebra $\mathfrak{o}(n)$ is the Lie algebra of $n \times n$ real matrices $X$ such as $X^T = -X$.
    \item The special orthogonal group $SO(n) \subset SL(n,\mathbb{R})$ is the subgroup of $O(n)$ with determinant $1$. It has the same Lie algebra as $O(n)$: $\mathfrak{so}(n) = \mathfrak{o}(n)$.
    \item The unitary group  $U(n)\subset GL(n,\mathbb{C})$ is the group of $n \times n$ complex matrices $\Omega$ such that $\Omega^\dagger = \Omega^{-1}$. Its Lie algebra $\mathfrak{su}(n)$ is the Lie algebra of $n \times n$ complex anti-Hermitian \footnote{skew = anti} matrices $X$, those satisfying $X^\dagger = -X$.
    \item The special unitary group $SU(n)\subset SL(n,\mathbb{C})$ is the subgroup of $U(n)$ matrices with determinant 1. Its Lie algebra $\mathfrak{su}(n)$ is the Lie algebra of $n \times n$ anti-Hermitian  traceless matrices $X$.
\end{itemize}
\end{document}
